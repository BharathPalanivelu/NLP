Project 3: Neural Transliteration

# Overview
This project aims to give you some hands on experience with neural models for NLP, and to put into practice some of the concepts discussed in lectures, readings, and recent paper presentations. 

The goal of the project is to perform transliteration, i.e. convert words (typically names) from one orthographic system to another.  For instance, "Beijing" is the English transliteration of the Chinese name "±±¾©". Transliteration is not a deterministic process.  Rules are the product of conventions that vary over time, depend on the origin of words and other historical accidents. For instance "Peking" is sometimes also used as the transliteration of  "±±¾©".  Transliteration can be used as a component in machine translation, to provide a target language form for out-of-vocabulary source names. 
In this project, we will focus on transliteration from Bulgarian, which is written with the Cyrillic alphabet, into English. We will use a sequence-to-sequence neural model. It will take as input a sequence of characters in the input language and produce a sequence of characters in the output language.

# Set-up 
You will need to install the following python libraries: numpy, matplotlib, nltk and pytorch. Instructions to install pytorch on your machine can be found here: http://pytorch.org/ (Links to an external site.). We used python 2.7 when testing the project.

Download p3.tgzView in a new window

# Submission 
You will submit a single zip file that contains the following:

noattention.py (see Q7)
any modified or new files created in answer to Q11 
report.pdf which contains the text and results (tables and/or figures) in answer to all questions
a file  team.txt that includes the names of the team members and a contact email. The project is to be done in groups of 3.
 

Understanding the sequence-to-sequence model implementation
Test out the installation by training the transliteration model on the provided toy dataset.

`python transliterate.py -t data/en_bg.toy.txt -v data/en_bg.toy.txt -n 100 -o data/en_bg.toy.out`
Reading lines in data/en_bg.toy.txt
Read 10 word pairs
Vocabulary statistics
('bg', 23)
('en', 20)
Examples of output for a random sample of training examples
('INPUT: ', u'\u0410\u0431\u043b\u0430\u043d\u0438\u0446\u0430')
('TARGET: ', u'ablanitsa')
('OUTPUT: ', u'abla<EOS>')

('INPUT: ', u'\u0410\u0430\u0445\u0435\u043d')
('TARGET: ', u'aachen')
('OUTPUT: ', u'abla<EOS>')

Evaluate on unseen data
Read 10 word pairs
Keeping 10 word pairs for which all characters are in vocabulary
Average edit distance 4.1000
 
This trains a transliteration model on a toy dataset of 10 examples only for 100 iterations. When training is done, the script prints the transliterations generated by the model for a random sample of the training data, as a sanity check. Finally, we transliterate the examples in the validation set and evaluate performance of the model using the edit distance between system output and provided target. The score reported is the average edit distance per entry in the validation set. There are two output files: (1) the transliterations for the validation set are in data/en_bg.toy.out as specified on the command line, and (2) learningcurve.pdf shows how the loss changed over training time (there are not enough data points to draw a curve on the toy dataset, so don't worry if this file is empty.)

Now we are ready to train on the entire training set  

`python transliterate.py -t data/en_bg.train.txt -v data/en_bg.val.txt -n 20000 -o data/en_bg.val.out >& log`

- 12-12-2017, n=200, 20s, average edit distance=6.5747
- 12-12-2017, n=1000, 2m, average edit distance=5.3137
- 12-12-2017, n=5000, 5m52s, average edit distance=1.9642
- 12-12-2017, n=20000, 40m4s, average edit distance=1.0568

Running this until completion takes about 30 min on our machines.  Using default settings, the training loss is printed every 1000 iterations so you can track progress.  For reference, the loss we obtained at the end of the 20000 iterations of training was 0.4921, and the average edit distance on the validation set was 0.9739.

We will then look at specific sections of the code to understand the model and how it is trained. (Note: if you'd like to read up more on pytorch and understand the implementation beyond what is highlighted below, the code follows this tutorial for translation with sequence-to-sequence models (Links to an external site.).)

## Seq-to-seq model configuration

Take a look at the EncoderRNN and AttnDecoderRNN classes in transliterate.py to understand the model configuration used, and answer the following questions:

**Q1 [10%]**  Describe the network architecture for the encoder, and for the decoder model: what kind of RNNs are used? What are the dimensions of the various layers? What are the non-linearity functions used? How is the attention computed? 
- 

## Training algorithm

Now let's look at how the model is trained. The training process is controlled by the trainIter() function, which calls train() on each example.

**Q2 [5%]** Two important hyperparameters control training: n_iters and  learning_rate. Explain what role each of them plays
**Q3 [5%]** Select values for these hyperparameters using the validation set. Describe the experiments you ran to select those values, and explain your reasoning.

## Understanding teacher_forcing

The train() function makes use of the teacher_forcing variable which controls how the output is generated for a given input word.

**Q4 [5%]** Explain how training works if teacher_forcing is set to 0, and if teacher_forcing is set to 1.
- if teacher_forcing is set to 0, each input is generated purely by the decoder's prediction. If teacher forcing is set to 1, each input is generated by the real target output. 
- teacher_forcing = 0.5, original code
	12-12-2017, n=1000, 2m, average edit distance=5.3137
- teacher_forcing = 0
	12-16-2017, n=1000, 2m, average edit distance=5.9200
- teacher_forcing = 1
	12-12-2017, n=1000, 2m, average edit distance=6.3979
**Q5 [10%]** Investigate the impact of teacher forcing empirically. Report learning curves for 0.1, 0.5 and 0.9, and explain what you observe.
- teacher_forcing = 0.1, n = 20000
	[2.3387, 1.9905, 1.5397, 1.2137, 0.9929, 0.9576, 0.8789, 0.7760, 0.7253, 0.6976, 0.7232, 0.6736, 0.6240, 0.5944, 0.6480, 0.5433, 0.5290, 0.5942, 0.6180, 0.7629]
	Average edit distance 3.6695
- teacher_forcing = 0.1, n = 20000
	[2.3476, 2.0669, 1.7584, 1.5757, 1.4066, 1.3186, 1.1290, 1.0486, 1.0247, 0.9451, 0.8726, 0.8210, 0.7607, 0.8671, 0.8327, 0.7728, 0.7992, 0.6768, 0.7066, 0.7112]
	Average edit distance 1.3747
- teacher_forcing = 0.1, n = 20000
	[2.2995, 1.9816, 1.5649, 1.3289, 1.1001, 0.9964, 0.8743, 0.8303, 0.8845, 0.7591, 0.8033, 0.7171, 0.6942, 0.8227, 0.7500, 0.7547, 0.6546, 0.6959, 0.6682, 0.7362]
	Average edit distance 1.8716
- teacher_forcing = 0.5, n = 20000
	[2.3947， 1.9697， 1.6475， 1.3342， 1.0840， 1.0015， 0.8446， 0.9143， 0.7836， 0.6965， 0.6895， 0.6359， 0.6604， 0.7001， 0.6940， 0.6033， 0.5508， 0.5580， 0.5221， 0.5598]
	Average edit distance 1.3768
- teacher_forcing = 0.9, n = 20000
	[2.3920, 1.6271, 1.1771, 0.9673, 0.8539, 0.7060, 0.6642, 0.6957, 0.5591, 0.5726, 0.5595, 0.5413, 0.5247, 0.4880, 0.4763, 0.4659, 0.4825, 0.3906, 0.4391, 0.4774]
	Average edit distance 1.2421
- teacher_forcing = 0.9, n = 20000
	[2.3665, 1.6007, 1.0566， 0.8898， 0.8482， 0.7081， 0.6986， 0.5904， 0.6117， 0.5460， 0.4688， 0.5590， 0.4290， 0.4703， 0.4849， 0.4927， 0.5098， 0.4603， 0.4732， 0.4830]
	Average edit distance 1.2779



以下数据作废，用错了py文件，所有teacher_forcing都=0.5
- teacher_forcing = 0.5, n = 10000
	[2.4114, 1.9639, 1.4539, 1.2059, 1.0049, 0.9565, 0.9056, 0.8406, 0.7909, 0.6907]
- teacher_forcing = 0.5, n = 10000
	[2.4021, 1.9127, 1.4876, 1.1665, 1.1107, 0.8920, 0.8341, 0.8353, 0.8185, 0.7475]
- teacher_forcing = 0.5, n = 10000
	[2.3936, 1.9107, 1.4225, 1.1273, 1.0037, 0.9333, 0.8463, 0.8232, 0.8126, 0.8877]
- teacher_forcing = 0.5, n = 20000
	[2.4122, 1.9376, 1.4469, 1.1745, 0.9959, 0.9777, 0.8808, 0.8262, 0.7743, 0.6708, 0.7035, 0.7141, 0.6587, 0.5779, 0.5939, 0.5559, 0.5824, 1.7077, 1.7754, 1.4751]
	Average edit distance 3.9305
- teacher_forcing = 0.5, n = 20000
	[2.4343, 1.9439, 1.4215, 1.1259, 1.0630, 0.9392, 0.8825, 0.8610, 0.7840, 0.7479, 0.7683, 0.7556, 0.6608, 0.5998, 0.6119, 0.5627, 0.8250, 0.6799, 0.6051, 0.6240]
	Average edit distance 1.5874
- teacher_forcing = 0.5, n = 20000
	[2.3947， 1.9697， 1.6475， 1.3342， 1.0840， 1.0015， 0.8446， 0.9143， 0.7836， 0.6965， 0.6895， 0.6359， 0.6604， 0.7001， 0.6940， 0.6033， 0.5508， 0.5580， 0.5221， 0.5598]
	Average edit distance 1.3768

## Impact of attention mechanism

**Q6 [5%]** Based on what we have learned in class, formulate a hypothesis about why the attention model is useful to model transliteration.
**Q7 [10%]**  Update the implementation you have been given to use a sequence-to-sequence model without attention.  This will require modifying the code in transliterate.py to use DecoderRNN instead of AttnDecoderRNN.  Submit the modified code in a file named noattention.py
**Q8 [10%]** Evaluate whether your hypothesis holds by comparing the behavior of the sequence-to-sequence model with and without attention empirically, at training and test time.  

## Now try something new
Finally, the goal is for you to be creative and try something new, taking inspiration from the readings on neural MT (Links to an external site.) as well as papers that were presented in the last few weeks. You can try to improve the performance of the transliterator,  its training speed, its robustness, or to expand its capabilities by handling multiple languages.

Here are some potential directions to get you thinking (ranked roughly from easier to harder):

What kind of errors does the current model make? If translations are too long or too short, perhaps better length models would help. Or perhaps use ensemble of models to make predictions would improve generalization.

Can the model architecture be improved? Perhaps different encoder or decoder architectures are better suited to modeling transliteration?

What could be done to use more data?  you could collect data from related languages and train a multilingual model as in the Google paper we read in class. Or you could pre-train character embeddings or encoder/decoder RNNs on data from a single language.

Perhaps the training set-up could be improved in other ways, by using edit distance more directly in training via reinforcement learning or minimum risk training.

Other ideas are more than welcome, check with instructors if you would like some feedback.

**Q9 [15%]**  Briefly explain what you did 
Define the problem that you are addressing
Explain why this problem matters
Describe your proposed solution
Explain how your solution addresses the problem 
**Q10 [15%]**  Design an experiment to test whether your solution successfully addresses the problem (e.g., compare the performance of your new model with the baseline system on the validation set, as well as the learning curves.). Present and discuss your results.  If the results are unexpected, explain what you think went wrong, and provide supporting analysis. 
**Q11 [10%]**  Provide the implementation so we can replicate your results. You can create additional files as needed as long as they are in the p3 directory. Instructions for running the code should be provided in the readme file. Note that it is your responsibility to test your code and make sure the instructions are accurate and self-contained.  If the code doesn't run, we will not attempt to debug it.  By default, we will evaluate by running the following command to train a transliteration model, and evaluate it on held out test data:
 
`python transliterate.py -t data/en_bg.train.txt -v data/en_bg.test.txt -n 20000 -o data/en_bg.test.out >& log `

**Extra-credit [up to 15%]**  will be used to reward groups that experiment with ambitious ideas that require substantially more work and deeper understanding of the model (e.g., successfully using reinforcement learning or minimum risk training to incorporate edit distance during training would lead to extra-credit.)

(Hints: if your idea requires modifying the model or the training regimen, you should really read this introduction to autograd (Links to an external site.) and to constructing neural networks with torch.nn (Links to an external site.))