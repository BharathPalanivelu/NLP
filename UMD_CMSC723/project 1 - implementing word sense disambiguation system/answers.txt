Team: 
1- Yue Liu
2- Binghuan Zhang

Email: liu2368@umd.edu, bz0045@terpmail.umd.edu

---------------------------------------------------------------------------------
Part 1)

1- baseline accuracy = 55.54%
2- Cohen’s Kappa = 1

---------------------------------------------------------------------------------
Part 2)
1- 

s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s)| 273  |   253      |   251     |  312  |   1526  |  287


s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,time)|  12  |    13      |    16     |  15   |    43   |  19

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,loss)|   1  |    0       |    0      |  2    |    23   |  0


s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
c(s,export)|   0  |    0       |    0      |  1    |    3    |    0



2-

s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s)| 0.09407305 | 0.08718125 | 0.08649207 | 0.10751206 | 0.52584425 | 0.09889731


s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|time)| 0.10169492 | 0.11016949 | 0.13559322 | 0.12711864 | 0.36440678 | 0.16101695

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|loss)| 0.03846154 | 0 | 0 | 0.07692308 | 0.88461538 | 0


s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
p(s|export)| 0 | 0 | 0 | 0.25 | 0.75 | 0


3- for the sentence X = "and i can tell you that i 'm an absolute nervous wreck every time she performs . i have her practice the last two lines on each page , so I can learn exactly when to turn the page -- just one of the tricks to this trade that i 've learned the hard way ."

s     | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|X)| 0 | 0 | 0 | 0 | 0 | 0

4- classifier f-measures on the test set:
micro averaged = 0.78330658105938999
macro averaged = 0.66173671317495919

5- We used multiple matrixs to store counts and log-probabilities. We applied add-1 smoothing and log-transformation. For 2.2 and 2.3, we didn't apply add-1 smoothing.

---------------------------------------------------------------------------------
Part 3)

1- ``_phone:-1.0,did_phone:-1.0,hauer_phone:-1.0,last_phone:-1.0,harp_phone:-1.0,down_phone:-1.0,the_phone:-2.0,of_phone:-1.0,room_phone:-1.0,tied_phone:-1.0,plucky_phone:-1.0,to_phone:-2.0,rolland_phone:-1.0,an_phone:-1.0,french_phone:-1.0,sits_phone:-1.0,._phone:-1.0,friend_phone:-1.0,a_phone:-1.0,lady_phone:-1.0,another_phone:-1.0,madame_phone:-1.0,she_phone:-1.0,managed_phone:-1.0,liberty_phone:-1.0,wordsworth_phone:-1.0,dog_phone:-1.0,ah_phone:-1.0,before_phone:-1.0,shows_phone:-1.0,drawing_phone:-1.0,by_phone:-1.0,her_phone:-2.0,,_phone:-3.0,not_phone:-1.0,in_phone:-1.0,!_phone:-1.0,lafayette_phone:-1.0,and_phone:-1.0,who_phone:-1.0,jean-jacques_phone:-1.0,plank_phone:-1.0,painting_phone:-1.0,exquisite_phone:-1.0,little_phone:-1.0,line_phone:-1.0,pet_phone:-1.0,much-quoted_phone:-1.0,with_phone:-1.0,offsetfeature_phone=-1.0,``_text:1.0,did_text:1.0,hauer_text:1.0,last_text:1.0,harp_text:1.0,down_text:1.0,the_text:2.0,of_text:1.0,room_text:1.0,tied_text:1.0,plucky_text:1.0,to_text:2.0,rolland_text:1.0,an_text:1.0,french_text:1.0,sits_text:1.0,._text:1.0,friend_text:1.0,a_text:1.0,lady_text:1.0,another_text:1.0,madame_text:1.0,she_text:1.0,managed_text:1.0,liberty_text:1.0,wordsworth_text:1.0,dog_text:1.0,ah_text:1.0,before_text:1.0,shows_text:1.0,drawing_text:1.0,by_text:1.0,her_text:2.0,,_text:3.0,not_text:1.0,in_text:1.0,!_text:1.0,lafayette_text:1.0,and_text:1.0,who_text:1.0,jean-jacques_text:1.0,plank_text:1.0,painting_text:1.0,exquisite_text:1.0,little_text:1.0,line_text:1.0,pet_text:1.0,much-quoted_text:1.0,with_text:1.0，offsetfeature_text=1.0
2- comma separated accuracies (e.g. 30,35,60): 0.79979324603721569, 0.89248793935217097, 0.92694693314955201, 0.93246037215713307, 0.82494831150930403, 0.89627842866988294, 0.98518263266712613, 0.9755341144038594, 0.99207443142660234, 0.98862853204686418, 0.99345279117849761, 0.93831840110268783, 0.99655410062026184, 0.99724328049620947, 0.99896623018607855, 0.99483115093039287, 1.0, 1.0, 1.0, 1.0
3- classifier f-measures on the test set:
micro averaged = 0.8378812199036918
macro averaged = 0.74574630451738244
4- we have the following implementation choices: number of iterations= 20, no random shuffling of examples, no weight averaging and learning rate =1 
---------------------------------------------------------------------------------
Part 4)
A) Feature A: position of "line"

1- Description: the position (in percentage) of the first appeared word "line" in the sentence. In naive bayes, the percentages are grouped into 5 categories ranging from 0 to 1. 

2- naive-bayes f-measures on the test set:
micro averaged = 0.78330658105938999
macro averaged = 0.66107955729622081

3- perceptron f-measures on the test set:
micro averaged = 0.81380417335473521
macro averaged = 0.71682171204623468


4- Conclusions: Adding the new feature didn't improve the nave bayes base algorithm. It also does not work for perceptron model and both of micro-and macro-averaged F1-score are decreased by 2%-3%.

B) Feature B: stopwords

1- Description: ratio (in percentage) of stopwords vs. non-stopwords in each sentence. The selection of stopwords is based on the stopwords module in nltk. In naive bayes, the percentages are grouped into 5 categories ranging from 0 to 1. Add-1 smoothing is also applied to eliminate zero counts.

2- naive-bayes f-measures on the test set:
micro averaged = 0.78651685393258419
macro averaged = 0.66778711768645616

3- perceptron f-measures on the test set:
micro averaged = 0.8041733547351525
macro averaged = 0.70506088797737576


4- Conclusions: Adding the new feature also improved the base algorithm just a little bit. It's also worth exploring. It does not work for perceptron model and both of micro-and macro-averaged F1-score are decreased by 3%-4%.



